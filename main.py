# main.py (ì´ˆê¸° ë¡œë”© ì•ˆì •í™” ìµœì¢… ë²„ì „)
import asyncio
import os
import re
import time
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from playwright.async_api import async_playwright, Page, Browser, TimeoutError
from typing import Union
from money_parser import to_won

app = FastAPI()


class LowestPriceDto(BaseModel):
    address: str
    salePrice: Union[int, None] = None
    rentPrice: Union[int, None] = None
    sourceUrl: Union[str, None] = None
    error: Union[str, None] = None


async def extract_price(page: Page) -> Union[int, None]:
    try:
        await page.wait_for_selector(".price-info-area", timeout=10000)
        label = page.locator("*:has-text('ë§¤ë¬¼ ìµœì €ê°€')").first
        if await label.count() > 0:
            price_area = label.locator("..").locator(".price-info-area .price-area .txt")
            if await price_area.count() > 0:
                price_text = await price_area.first.text_content()
                if price_text and ('ì–µ' in price_text or 'ë§Œ' in price_text):
                    return to_won(price_text.strip())
        price_elements = await page.locator(".price-info-area .price-area .txt").all()
        for el in price_elements:
            price_text = await el.text_content()
            if price_text and ('ì–µ' in price_text or 'ë§Œ' in price_text):
                price = to_won(price_text.strip())
                if price > 0:
                    return price
    except Exception as e:
        print(f"ê°€ê²© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    return None


async def fetch_lowest_by_address(address: str) -> LowestPriceDto:
    start_time = time.time()
    print(f"ğŸš€ '{address}' í¬ë¡¤ë§ ì‹œì‘...")

    async with async_playwright() as p:
        proxy_host, proxy_port, proxy_username, proxy_password = (
            os.getenv("PROXY_HOST"),
            os.getenv("PROXY_PORT"),
            os.getenv("PROXY_USERNAME"),
            os.getenv("PROXY_PASSWORD"),
        )
        proxy_settings = None
        if proxy_host and proxy_port:
            server = f"http://{proxy_host}:{proxy_port}"
            proxy_settings = {"server": server, "username": proxy_username, "password": proxy_password}

        browser: Browser = await p.chromium.launch(headless=True, args=["--no-sandbox"], proxy=proxy_settings)
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            ignore_https_errors=True
        )
        page: Page = await context.new_page()
        base_url = "https://www.bdsplanet.com"

        try:
            print(f"[{time.time() - start_time:.2f}s] ğŸ” ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹œë„...")
            # âœ¨ [ìˆ˜ì • 1] í˜ì´ì§€ ë¡œë”©ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³ , ì¼ë‹¨ ì ‘ì†ë§Œ ì‹œë„í•©ë‹ˆë‹¤.
            await page.goto(f"{base_url}/main.ytp", timeout=60000)
            print(f"[{time.time() - start_time:.2f}s] âœ… í˜ì´ì§€ ê¸°ë³¸ ë¡œë”© ì™„ë£Œ.")

            # âœ¨ [ìˆ˜ì • 2] í˜ì´ì§€ì˜ í•µì‹¬ ìš”ì†Œì¸ 'ê²€ìƒ‰ì°½'ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
            search_input_selector = "input[placeholder*='ì£¼ì†Œ'], input[placeholder*='ê²€ìƒ‰']"
            search_input = page.locator(search_input_selector).first
            await search_input.wait_for(state="visible", timeout=60000)
            print(f"[{time.time() - start_time:.2f}s] âœ… ê²€ìƒ‰ì°½ í‘œì‹œ í™•ì¸.")

            await asyncio.sleep(1)  # í˜ì´ì§€ ìŠ¤í¬ë¦½íŠ¸ê°€ ì•ˆì •í™”ë  ì‹œê°„ì„ ì¤ë‹ˆë‹¤.

            try:
                ad_pop_selector = ".pop.adPop"
                await page.wait_for_selector(ad_pop_selector, state="visible", timeout=7000)
                await page.evaluate(f"document.querySelector('{ad_pop_selector}').remove();")
                print(f"[{time.time() - start_time:.2f}s] âœ… ê´‘ê³  íŒì—… ì œê±° ì™„ë£Œ.")
            except TimeoutError:
                print(f"[{time.time() - start_time:.2f}s] â„¹ï¸ ê´‘ê³  íŒì—… ê°ì§€ë˜ì§€ ì•ŠìŒ.")

            await search_input.fill(address)

            autocomplete_selector = ".ui-autocomplete .ui-menu-item"
            await page.wait_for_selector(autocomplete_selector, timeout=10000)
            await page.locator(autocomplete_selector).first.click()
            print(f"[{time.time() - start_time:.2f}s] âœ… ê²€ìƒ‰ ì‹¤í–‰ ì™„ë£Œ.")

            final_url_pattern = re.compile(r"/map/realprice_map/[^/]+/N/[ABC]/")
            await page.wait_for_url(final_url_pattern, timeout=60000)
            final_url = page.url
            print(f"[{time.time() - start_time:.2f}s] âœ… ìµœì¢… URL ë„ì°©: {final_url}")

            match = re.search(r"(/map/realprice_map/[^/]+/N/[ABC]/)([12])(/[^/]+\.ytp.*)", final_url)
            if match:
                base_pattern, _, suffix = match.groups()
                sale_url = f"{base_url}{base_pattern}1{suffix}"
                rent_url = f"{base_url}{base_pattern}2{suffix}"

                await page.goto(sale_url, wait_until="domcontentloaded")
                sale_price = await extract_price(page)

                await page.goto(rent_url, wait_until="domcontentloaded")
                rent_price = await extract_price(page)

                return LowestPriceDto(address=address, salePrice=sale_price, rentPrice=rent_price, sourceUrl=sale_url)
            else:
                return LowestPriceDto(address=address, error=f"URL íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {final_url}")

        except Exception as e:
            return LowestPriceDto(address=address, error=f"í¬ë¡¤ë§ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            await context.close()
            await browser.close()


@app.get("/crawl", response_model=LowestPriceDto)
async def crawl_real_estate(address: str):
    if not address:
        raise HTTPException(status_code=400, detail="ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    return await fetch_lowest_by_address(address)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)